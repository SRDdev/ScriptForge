{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ScriptGPT\nVery much inspired by Andrej Karpathy's [minGPT](https://github.com/karpathy/minGPT).\nThis notebook is a demo version for training a GPT model from pretrained Huggingface Models.\nThis model is available on Huggingface Hub as [ScriptGPT](https://huggingface.co/SRDdev/Script_GPT)\n\n### Notes\nIn this notebook we will be training a Generative Pre Trained Transformer model from Huggingface models.We will not be training it from scratch as I personally do not have that much computation power.","metadata":{}},{"cell_type":"markdown","source":"### Logging into ðŸ¤—Huggingface\n\nLog into your Huggingface account.\n\nIf you don't have an account then you can make one for [free](https://huggingface.co/).","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"id":"JQX8uqZPbtOE","outputId":"0fefc56a-3f0a-4cbc-88b2-fac5abed89cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then you need to install Git-LFS. Uncomment the following instructions:","metadata":{"id":"VnNkO39obtOE"}},{"cell_type":"code","source":"!apt install git-lfs","metadata":{"id":"vnJZelclbtOE","outputId":"25c99df1-25f9-4ab6-c3b8-bf1d3331770c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:","metadata":{"id":"JHfqGIRhbtOF"}},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"id":"x7yBBEwPbtOF","outputId":"1ebd1e86-517e-4505-cf0b-811845e05ced","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data\n\nLoad the data from [kaggle](https://www.kaggle.com/datasets/jfcaro/5000-transcripts-of-youtube-ai-related-videos).\n\nThen we will split the entire dataset into multiple files containing 10000 lines. We are doing this as the computation power available is very limited. You can try to increase the number of lines in a single file.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"jamescalam/youtube-transcriptions\")","metadata":{"id":"1ShHPaTkHwt4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = list(dataset['train']['start'])\nend_time = list(dataset['train']['end'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict\n\ndef merge_videos(dataset):\n    merged_list = []\n    prev_title = ''\n    prev_text = ''\n\n    for row in dataset['train']:\n        title = row['title']\n        text = row['text']\n        if title != prev_title:\n            # Start of a new video\n            if prev_title:\n                # Add the merged text for the previous video to the list\n                merged_list.append({'title': prev_title, 'text': prev_text})\n            prev_title = title\n            prev_text = text\n        else:\n            # Same title as previous row, append the text\n            prev_text += ' ' + text\n\n    # Add the merged text for the last video to the list\n    if prev_title:\n        merged_list.append({'title': prev_title, 'text': prev_text})\n\n    # Create a new dataset with the merged text for each title\n    merged_dataset = DatasetDict({'train': merged_list})\n\n    return merged_dataset\n\nmerged_dataset = merge_videos(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_text_file(merged_dataset, filename):\n    with open(filename, 'w', encoding='utf-8') as f:\n        for row in merged_dataset['train']:\n            title = row['title']\n            text = row['text']\n            f.write(f'{title} : {text}\\n')\n            \nwrite_text_file(merged_dataset, 'merged_text.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/merged_text.txt', 'r', encoding='utf-8') as f:\n    num_lines = sum(1 for _ in f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_lines","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = data.drop(\"author\",axis=1)\n# data = data.drop(\"playlist_name\",axis=1)\n# data['script'] = data['title'] + '\\t' + data['transcript']\n# data = data.drop(\"title\",axis=1)\n# data = data.drop(\"transcript\",axis=1)\n# data.to_csv('path_to_train.txt', sep='\\t', index=False)","metadata":{"id":"AHC3rM5LImqD","outputId":"d711a150-8a5a-4ef1-e68a-b23031ab0891","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# specify the path to your input file\ninput_file = \"/kaggle/working/merged_text.txt\"\n\n# specify the directory to save the output files in\noutput_dir = \"/kaggle/working/\"\n\n# create the output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\n# open the input file for reading\nwith open(input_file, \"r\") as f:\n    # initialize a counter to keep track of the number of lines\n    line_count = 0\n    # initialize a file counter to keep track of the number of output files\n    file_count = 0\n    # initialize a file object for the first output file\n    current_file = open(os.path.join(output_dir, f\"output_{file_count}.txt\"), \"w\")\n    # iterate over each line in the input file\n    for line in f:\n        # write the line to the current output file\n        current_file.write(line)\n        # increment the line count\n        line_count += 1\n        # if we've written 500 lines to the current output file, close it and open a new one\n        if line_count == 500:\n            current_file.close()\n            file_count += 1\n            current_file = open(os.path.join(output_dir, f\"output_{file_count}.txt\"), \"w\")\n            # reset the line count to 0\n            line_count = 0\n    # close the last output file\n    current_file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely.","metadata":{"id":"7hY7IeT5btOG"}},{"cell_type":"code","source":"from transformers.utils import send_example_telemetry\n\nsend_example_telemetry(\"language_modeling_notebook\", framework=\"pytorch\")","metadata":{"id":"0GBsA_oEbtOH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Huggingface Datasets\n\nIn this section we will create a Huggingface Dataset from our split data. This is must as HF model require the input data in cretain format only.","metadata":{}},{"cell_type":"code","source":"# !pip install datasets","metadata":{"id":"3Vf8Ar2SKlpA","outputId":"20caae9d-4ae2-4084-af0c-a0061a3954c1","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndatasets = load_dataset(\"text\", data_files={\"train\": \"/kaggle/working/output_0.txt\",\"validation\":\"/kaggle/working/output_1.txt\"})","metadata":{"id":"uxSaGa_l3l-W","outputId":"3dd71541-f089-430e-fe6a-d9207af575f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets","metadata":{"id":"ClW_oVX4ckxj","outputId":"0f582dfc-e807-4317-be8e-5a50437aca41","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.","metadata":{"id":"WHUmphG3IrI3"}},{"cell_type":"code","source":"from datasets import ClassLabel\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\n\ndef show_random_elements(dataset, num_examples=2):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n    display(HTML(df.to_html()))","metadata":{"id":"ur5sNUcZ3l-g","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_random_elements(datasets[\"train\"])","metadata":{"id":"1Uk8NROQ3l-k","outputId":"b8c9ea24-709f-4f24-a21e-81f50fb15457","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Causal Language modeling\n\nFor causal language modeling (CLM) we are going to take all the texts in our dataset and concatenate them after they are tokenized. Then we will split them in examples of a certain sequence length. This way the model will receive chunks of contiguous text that may look like:\n\nWe will use [ScriptGPT-small](https://huggingface.co/SRDdev/Script_GPT) which is pre-trained on similar scripting dataset, but you can also use [gpt2](https://huggingface.co/gpt2)","metadata":{"id":"JEA1ju653l-p"}},{"cell_type":"code","source":"model_checkpoint = \"gpt2\"","metadata":{"id":"-WGBCO343l-q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenizer\nTo tokenize all our texts with the same vocabulary that was used when training the model, we have to download a pretrained tokenizer. This is all done by the `AutoTokenizer` class:","metadata":{"id":"5io6fY_d3l-u"}},{"cell_type":"code","source":"from transformers import AutoTokenizer   \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","metadata":{"id":"iAYlS40Z3l-v","outputId":"7f123e76-d33e-4f4a-d54c-d0d56b2f2fa4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:\n\nThen we apply it to all the splits in our `datasets` object, using `batched=True` and 4 processes to speed up the preprocessing. We won't need the `text` column afterward, so we discard it.\n\nIf we now look at an element of our datasets, we will see the text have been replaced by the `input_ids` the model will need:","metadata":{"id":"rpOiBrJ13l-y"}},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"])\n\ntokenizer.model_max_length = 2500\n\ntokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4,remove_columns=[\"text\"])\n\ntokenized_datasets[\"train\"][1]","metadata":{"id":"lS2m25YM3l-z","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain `block_size`. To do this, we will use the `map` method again, with the option `batched=True`. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n\nFirst, we grab the maximum length our model was pretrained with. This might be a big too big to fit in your GPU RAM, so here we take a bit less at just 128.","metadata":{"id":"obvgcXda3l--"}},{"cell_type":"code","source":"# block_size = tokenizer.model_max_length\nblock_size = 256","metadata":{"id":"DVHs5aCA3l-_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we write the preprocessing function that will group our texts:","metadata":{"id":"RpNfGiMw3l_A"}},{"cell_type":"code","source":"def group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"id":"iaAJy5Hu3l_B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First note that we duplicate the inputs for our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually.\n\nAlso note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:","metadata":{"id":"LGJWXtNv3l_C"}},{"cell_type":"code","source":"lm_datasets = tokenized_datasets.map(\n    group_texts,\n    batched=True,\n    batch_size=5000,\n    num_proc=4,\n)","metadata":{"id":"gXUSfBrq3l_C","outputId":"ed56ff71-be28-4795-d21a-c05f3f9db973","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can check our datasets have changed: now the samples contain chunks of `block_size` contiguous tokens, potentially spanning over several of our original texts.","metadata":{"id":"6n84V8Gc3l_G"}},{"cell_type":"code","source":"tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])","metadata":{"id":"hTeGCLl_3l_G","outputId":"511b2de6-37b5-4f0a-c4ef-1b9adbada24c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the data has been cleaned, we're ready to instantiate our `Trainer`. We will a model:","metadata":{"id":"iEmeQ7Xm3l_H"}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(model_checkpoint)","metadata":{"id":"sPqQA3TT3l_I","outputId":"ba36d211-520e-4f5b-fdd6-59aa5cc2a3a5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And some `TrainingArguments`:","metadata":{"id":"VyPQTOF_3l_J"}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# model_name = model_checkpoint.split(\"/\")[-1]\ntraining_args = TrainingArguments(\n    \"GPT2script\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    save_steps=5000, # Add save_steps parameter with value 500\n)\n\n\"\"\"\ntraining_args = TrainingArguments(\n    \"GPT2script\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n    save_steps=5000,\n    lr_scheduler_type=\"cosine\",\n    num_warmup_steps=1000,\n    num_training_steps=10000,\n    per_device_train_batch_size=4,\n)\n\"\"\"","metadata":{"id":"YbSwEhQ63l_L","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook.","metadata":{"id":"3vRzs5CXbtOQ"}},{"cell_type":"markdown","source":"We pass along all of those to the `Trainer` class:","metadata":{"id":"sZRbT9ui3l_N"}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"validation\"],\n)","metadata":{"id":"OEuqwIra3l_N","outputId":"425aaa68-73f4-45a4-d6aa-22851eb6ff8f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can train our model:","metadata":{"id":"6Vvz34Td3l_O"}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"NyZvu_MF3l_P","outputId":"70a9cfce-f1b4-4627-fb76-79ede05979b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:","metadata":{"id":"3APq-vUc3l_R"}},{"cell_type":"code","source":"import math\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"id":"diKZnB1I3l_R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface-cli","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can now upload the result of the training to the Hub, just execute this instruction:","metadata":{"id":"wY82caEX3l_i"}},{"cell_type":"markdown","source":"### Push to Hub & Pipeline\n\nNow we will push the final model to Huggingface Model Hub.\n- Model \n- Tokeizer\n- Trainer\n\nWe will then build a pipeline using our model for Hosted Inference using the transformers pipeline function.","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.push_to_hub(\"SRDdev/Script_GPT\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"SRDdev/Script_GPT\")","metadata":{"id":"HpM3v86NlGbQ","outputId":"509c0a20-4667-44a7-979a-6c05c11b7bfa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nGenerate = pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\nscript = Generate(\"Importing Keras models into TensorFlow.js\", max_length=1000, do_sample=True)","metadata":{"id":"NSNTAr83u8Te","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"script[0]['generated_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Zip download weights","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\n# Define the folder path and zip file name\nfolder_path = '/kaggle/working/Script_GPT'\nzip_file_name = 'ScriptGPT.zip'\n\n# Zip the folder\nwith zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(folder_path, '..')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}